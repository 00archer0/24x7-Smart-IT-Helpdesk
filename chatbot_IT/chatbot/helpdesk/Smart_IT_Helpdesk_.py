
# coding: utf-8

# #                                  24x7 Smart IT Helpdesk

# ## A application to solve or give answers to IT problems like a IT help Customer care.

# ### importing all the required libraries



import pandas as pd
from nltk.tokenize import regexp_tokenize,word_tokenize
from nltk.corpus import stopwords
import re
from gensim import models
from collections import Counter 



# ### changeing path to current working directory or to where excel files are exists

# ### importing the files from which answers will be fetched and preprocessed to 
# ### exctract unique words and create text to feed word2vec algorithm



path = ".\chatbot\helpdesk"
datatoans = pd.read_excel('.\chatbot\helpdesk\processTOans.xlsx')








# ### Removing unwanted stopwords.

# ### code to preprocess client query removing stopwords , speacial characters and lowercased


a = ['no','not','down','up']
abc = list('abcdefghijklmnopqrstuvwxyz')
stopwors = stopwords
stopwords = stopwors.words('english')
stopwords.extend(abc)

for z in a :
    stopwords.remove(z)
    
def preProcess_query(que) : 
    que = str(que)
    que = que.lower()
    que = re.sub(r"won’t",'would not',que)
    que = re.sub(r"isn’t",'is not',que)
    que = re.sub(r"can’t",'can not',que)
    que = re.sub(r"doesn’t",'does not',que)
    que = re.sub(r'[^a-zA-Z]',' ',que)
    que = regexp_tokenize(que,r'\w+')
    que = [word for word in que if word not in stopwords]
    que = ' '.join(que)
    return que    





def create_dic(datatoans):
    
    # here a text is generated by joining all question and answer to find the unique words. var = text
    # and word tokenized. var = input_
    # unique words are extracted from the text.  var  = unique_list
    # with no. of occurance of there unique word in text.  var = uni

    text = []
    text = ' '.join(map(preProcess_query,datatoans['ques'])) + ' '.join(map(preProcess_query,datatoans['ans']))
    unique_dic = Counter(text.split(' '))
    unique_words = list(unique_dic.keys())
    
    # empty dictonary
    dictonary = {}
    for word in unique_words:
        dictonary[word] = set()
        
    # dictonary populated
    for x in range(0,len(datatoans['ques'])):
        words = preProcess_query(datatoans['ques'][x]).split(' ')
        for y in words: 
            dictonary[y].add(str(x))
    print('dic_created')        
    return text,unique_words,dictonary 






def train(data,text):         
     # preparring data for Word2vec
    tokened = [None]*(len(data['ques'])+len(data['ans']))
    tokened[::2] = data['ques']
    tokened[1::2] = data['ans']
    tokened = [word_tokenize(preProcess_query(x)) for x in tokened ]
    
    #removing length 1 characters or special or numbers words from text
    # and creating a list of words without length 1 word.
    text_token = word_tokenize(text)
    for f in text_token:
        if len(f)==1:
            text_token.remove(f)
    # length 1 from questions and answers are removed
    for x in tokened:
        for y in x:
            if y not in text_token:
                x.remove(y)
       
    model = models.Word2Vec(tokened,batch_words=1000,min_count=1,size = 150,window=5,workers = 4 )
    model.train(tokened,total_examples=len(tokened),epochs=150)
    model.save('trained_model')
    print('model_trained')
    print('done')





text,unique_words,dictonary = create_dic(datatoans)
model = models.Word2Vec.load(path+'\\trained_model')





def best_match(indices,que):
 # ### code to finding most similar question in doc   
    indx = -1
    try:
        index_similarity = []
        for x in indices:
            index = int(x[0])           
            
            similarity = model.wv.n_similarity(que,preProcess_query(datatoans['ques'][index]).split())
            index_similarity.append((similarity,index))

        index_similarity.sort(reverse = True)
       
        indx = int(index_similarity[0][1])
    except Exception as e:
        print(e)
    return indx


# ### searching common questions from doc using unique word dictonary  which can be a participant to similar questions


# for retrive matching question from data
# getting maxmax_time_index 
def search(que):
    
    retrive = []
    max_time_index = []
    for term in que:
        if term in unique_words:
            
            retrive.append([index_no for index_no in dictonary[term]])
           
    
    join_retrived_indices = [y for x in retrive for y in x]
    
    matched =  sorted(Counter(join_retrived_indices).items(),key=lambda x :x[1],reverse=True)
    
    index = best_match(matched,que)
    
    if index == -1:
        ans = 'Model have to learn more vocabulary, Lack of Data or check your vocabulary'
    else:
        ans = datatoans['ans'][index]
   
    return ans,index



# ### code to processing response to client
def get_response(Question):
    que = preProcess_query(Question)
    que = que.split()
    print(que)
    que,que_index = search(que)
    #print('search \n'+'-'*20 + que)
    que = que.split('||')
    for x in que:
        if len(x)<3 or x == 0:
            que.remove(x)
    for x in range(0,len(que)):    
        que[x] = que[x].strip()
    #print('que \n'+'-'*20 + que)        
    #que.remove('')           
    return que,que_index





# ### add new question answer to database
questiontoadd = [] 
answertoadd = []
new = pd.DataFrame()

def melt_old_new(datatoans):
    new['ques'] = questiontoadd
    new['ans'] = answertoadd    
    datatoans = datatoans.append(new,ignore_index=True)
    datatoans.to_csv('datatoans_new.csv',index=False)
    return datatoans

def quesToDatabase(ques,ans,datatoans_old=datatoans,text=text,unique_words=unique_words,dictonary=dictonary,model=model):
    if len(questiontoadd)!=10:        
        questiontoadd.append(ques) 
        answertoadd.append(ans + " ||")
        text,unique_words,dictonary,model 
    else:
        datatoans_new = melt_old_new(datatoans_old)
        text,unique_words,dictonary = create_dic(datatoans_new)
        train(datatoans_new,text)
        model = models.Word2Vec.load('trained_model')
    return text,unique_words,dictonary,model




# ### feedback code to update the order of best resolution

def feedback(best_res,index):
    q = datatoans['ans'][index].split('||')
    for x in q:
        if len(x)<3 or x == 0:
            q.remove(x)
    for x in range(0,len(q)):    
        q[x] = q[x].strip()
    q.remove(best_res)
    q.insert(0,best_res)
    q  = '||'.join(q)
    datatoans['ans'][index] = str(q)
    return q

# ### to save final changes

def save_changes():
    datatoans.to_csv('datatoans_new.csv',index=False)
    return 'done'
    
